# Big Data Challenge - - "Alexa, can you handle big data?"
![Header](images/header.jpg)<br>

Welcome to my project repo! 
Feel free to take a look around the repo folders!<br>
This project is all about ETL processes in Big Data using PySpark and AWS Relational Databases. 

If you are interested, you can find out more information about the data here used:<br>
* [Amazon Reviews](https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt)<br><br>

## The Process
<hr>
The overall goal of this project is to be able to use PySpark, SQL and AWS Relational Database's to be able to complete an ETL process, and match dataframes to easily push data into a database from a notebook file. 
This project utilises a Free AWS Account and Database; you can sign up for a free account here:

[AWS Free Tier](https://aws.amazon.com/free), but remember - when you sign up to AWS you are required to enter your credit card details - so ensure you understand what you are agreeing to when you create instances within AWS! 
<br>
<br>
In the repo folders, you will find the SQL schema's used to create each of the tables in the database, along with select queries to ensure the data had transferred correctly.  
You will also find the two notebook files, used to ETL the data and ensure it was cleaned and in the correct format to be pushed to the database. 
<br><br>

### Created With<hr>
This project was created using the following:<br>
* Python
* PySpark
* PostgreSQL
* AWS Relational Database
* pgAdmin
* Google Colab
* Jupyter Notebooks





